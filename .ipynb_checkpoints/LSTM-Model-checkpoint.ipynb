{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string, os \n",
    "import warnings\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import Sequential\n",
    "import tensorflow.keras.utils as ku \n",
    "\n",
    "from numpy.random import seed\n",
    "import tensorflow\n",
    "tensorflow.random.set_seed(4)\n",
    "seed(20)\n",
    "os.chdir(\"../quoteAIsData\")\n",
    "\n",
    "def writeToFile(filename, data):\n",
    "    file = open(filename, 'w', encoding = \"utf-8\")\n",
    "    file.write(data)\n",
    "    file.close()\n",
    "    \n",
    "def loadFromFile(filename):\n",
    "    file = open(filename, 'r', encoding = \"utf-8\")\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DAVID 108324\n",
      "DIEGO 106373\n",
      "ERIC 1249063\n",
      "GWYN 1312330\n",
      "JETT 181826\n",
      "MARS 695638\n",
      "MILES 791200\n",
      "MILO 886187\n",
      "PARSA 339260\n"
     ]
    }
   ],
   "source": [
    "quote_dict = pickle.load(open('quotes.pkl', 'rb'))\n",
    "corpus_dict = {}\n",
    "for k in quote_dict:\n",
    "    corpus = ' '.join(quote_dict[k])\n",
    "    corpus_dict[k] = corpus\n",
    "    print(k, len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DAVID\n",
      "Total Tokens: 19020\n",
      "Unique Tokens: 3960\n",
      "DIEGO\n",
      "Total Tokens: 19084\n",
      "Unique Tokens: 3077\n",
      "ERIC\n",
      "Total Tokens: 231264\n",
      "Unique Tokens: 15618\n",
      "GWYN\n",
      "Total Tokens: 149662\n",
      "Unique Tokens: 10075\n",
      "JETT\n",
      "Total Tokens: 33283\n",
      "Unique Tokens: 5291\n",
      "MARS\n",
      "Total Tokens: 128621\n",
      "Unique Tokens: 10492\n",
      "MILES\n",
      "Total Tokens: 121145\n",
      "Unique Tokens: 10124\n",
      "MILO\n",
      "Total Tokens: 163472\n",
      "Unique Tokens: 11676\n",
      "PARSA\n",
      "Total Tokens: 63888\n",
      "Unique Tokens: 7952\n",
      "DAVID - 18969 sequences\n",
      "DIEGO - 19033 sequences\n",
      "ERIC - 231213 sequences\n",
      "GWYN - 149611 sequences\n",
      "JETT - 33232 sequences\n",
      "MARS - 128570 sequences\n",
      "MILES - 121094 sequences\n",
      "MILO - 163421 sequences\n",
      "PARSA - 63837 sequences\n"
     ]
    }
   ],
   "source": [
    "token_dict = {}\n",
    "for key, corpus in corpus_dict.items():\n",
    "    # split into tokens by white space\n",
    "    tokens = corpus.split()\n",
    "    # remove punctuation from each token\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # make lower case\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    token_dict[key] = tokens\n",
    "    print(key)\n",
    "    print('Total Tokens: %d' % len(tokens))\n",
    "    print('Unique Tokens: %d' % len(set(tokens)))\n",
    "\n",
    "for key, tokens in token_dict.items(): \n",
    "    length = 51\n",
    "    sequences = list()\n",
    "    for i in range(length, len(tokens)):\n",
    "        seq = tokens[i-length:i]\n",
    "        line = ' '.join(seq)\n",
    "        sequences.append(line)\n",
    "    print(f\"{key} - {len(sequences)} sequences\")\n",
    "    writeToFile(key+\"_sequences.txt\", '\\n'.join(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel(name):\n",
    "    in_filename = name + \"_sequences.txt\"\n",
    "    file = loadFromFile(in_filename)\n",
    "    lines = file.split('\\n')\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    sequences = tokenizer.texts_to_sequences(lines)\n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "    print(f\"{name}: {vocab_size} vocabulary size\")\n",
    "    pickle.dump(tokenizer, open(name + '_tokenizer.pkl', 'wb'))\n",
    "    sequences = np.array(sequences)\n",
    "    X, y = sequences[:,:-1], sequences[:,-1]\n",
    "    y = np_utils.to_categorical(y, num_classes=vocab_size)\n",
    "    seq_length = X.shape[1]\n",
    "    # define model\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, 50, input_length=seq_length))\n",
    "    model.add(LSTM(200, return_sequences=True))\n",
    "    model.add(LSTM(100))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(vocab_size, activation='softmax'))\n",
    "    print(model.summary())\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    # fit model\n",
    "    model.fit(X, y, batch_size=128, epochs=100)\n",
    "    model.save(name + '_char_word_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training PARSA bot\n",
      "PARSA: 7953 vocabulary size\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 50, 50)            397650    \n",
      "_________________________________________________________________\n",
      "lstm_8 (LSTM)                (None, 50, 200)           200800    \n",
      "_________________________________________________________________\n",
      "lstm_9 (LSTM)                (None, 100)               120400    \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 7953)              803253    \n",
      "=================================================================\n",
      "Total params: 1,532,203\n",
      "Trainable params: 1,532,203\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "499/499 [==============================] - 92s 184ms/step - loss: 7.0310 - accuracy: 0.0319\n",
      "Epoch 2/100\n",
      "499/499 [==============================] - 95s 190ms/step - loss: 6.8194 - accuracy: 0.0316\n",
      "Epoch 3/100\n",
      "499/499 [==============================] - 92s 185ms/step - loss: 6.7298 - accuracy: 0.0329\n",
      "Epoch 4/100\n",
      "499/499 [==============================] - 93s 186ms/step - loss: 6.6112 - accuracy: 0.0428\n",
      "Epoch 5/100\n",
      "499/499 [==============================] - 92s 184ms/step - loss: 6.4362 - accuracy: 0.0506\n",
      "Epoch 6/100\n",
      "499/499 [==============================] - 93s 187ms/step - loss: 6.2997 - accuracy: 0.0576\n",
      "Epoch 7/100\n",
      "499/499 [==============================] - 92s 185ms/step - loss: 6.1983 - accuracy: 0.0664\n",
      "Epoch 8/100\n",
      "499/499 [==============================] - 92s 184ms/step - loss: 6.0985 - accuracy: 0.0741\n",
      "Epoch 9/100\n",
      "499/499 [==============================] - 92s 184ms/step - loss: 6.0114 - accuracy: 0.0800\n",
      "Epoch 10/100\n",
      "499/499 [==============================] - ETA: 0s - loss: 5.9395 - accuracy: 0.08 - 93s 186ms/step - loss: 5.9395 - accuracy: 0.0862\n",
      "Epoch 11/100\n",
      "499/499 [==============================] - 94s 189ms/step - loss: 5.8531 - accuracy: 0.0921\n",
      "Epoch 12/100\n",
      "499/499 [==============================] - 94s 188ms/step - loss: 5.7734 - accuracy: 0.0984\n",
      "Epoch 13/100\n",
      "499/499 [==============================] - 94s 189ms/step - loss: 5.6911 - accuracy: 0.1035\n",
      "Epoch 14/100\n",
      "499/499 [==============================] - 95s 190ms/step - loss: 5.6200 - accuracy: 0.1064\n",
      "Epoch 15/100\n",
      "499/499 [==============================] - 95s 190ms/step - loss: 5.5629 - accuracy: 0.1103\n",
      "Epoch 16/100\n",
      "499/499 [==============================] - 96s 192ms/step - loss: 5.5085 - accuracy: 0.1129\n",
      "Epoch 17/100\n",
      "499/499 [==============================] - 94s 188ms/step - loss: 5.4321 - accuracy: 0.1155s - loss: 5.4314 - ac\n",
      "Epoch 18/100\n",
      "499/499 [==============================] - 92s 184ms/step - loss: 5.3568 - accuracy: 0.1190\n",
      "Epoch 19/100\n",
      "499/499 [==============================] - 91s 182ms/step - loss: 5.2834 - accuracy: 0.1216\n",
      "Epoch 20/100\n",
      "499/499 [==============================] - 94s 189ms/step - loss: 5.2377 - accuracy: 0.1233\n",
      "Epoch 21/100\n",
      "499/499 [==============================] - 97s 195ms/step - loss: 5.1733 - accuracy: 0.1259\n",
      "Epoch 22/100\n",
      "499/499 [==============================] - 95s 190ms/step - loss: 5.0987 - accuracy: 0.1294\n",
      "Epoch 23/100\n",
      "499/499 [==============================] - 97s 193ms/step - loss: 5.0259 - accuracy: 0.1314\n",
      "Epoch 24/100\n",
      "499/499 [==============================] - 94s 189ms/step - loss: 4.9499 - accuracy: 0.1350\n",
      "Epoch 25/100\n",
      "499/499 [==============================] - 95s 190ms/step - loss: 4.8814 - accuracy: 0.1390\n",
      "Epoch 26/100\n",
      "499/499 [==============================] - 95s 191ms/step - loss: 4.8168 - accuracy: 0.1417\n",
      "Epoch 27/100\n",
      "499/499 [==============================] - 97s 195ms/step - loss: 4.7715 - accuracy: 0.1445\n",
      "Epoch 28/100\n",
      "499/499 [==============================] - ETA: 0s - loss: 4.7058 - accuracy: 0.14 - 95s 191ms/step - loss: 4.7058 - accuracy: 0.1492\n",
      "Epoch 29/100\n",
      "499/499 [==============================] - 94s 189ms/step - loss: 4.6449 - accuracy: 0.1516\n",
      "Epoch 30/100\n",
      "499/499 [==============================] - 95s 190ms/step - loss: 4.5986 - accuracy: 0.1549\n",
      "Epoch 31/100\n",
      "499/499 [==============================] - 96s 193ms/step - loss: 4.5269 - accuracy: 0.1584\n",
      "Epoch 32/100\n",
      "499/499 [==============================] - 95s 190ms/step - loss: 4.5400 - accuracy: 0.1588\n",
      "Epoch 33/100\n",
      "499/499 [==============================] - 93s 187ms/step - loss: 4.4414 - accuracy: 0.1642\n",
      "Epoch 34/100\n",
      "499/499 [==============================] - 96s 192ms/step - loss: 4.3864 - accuracy: 0.1680\n",
      "Epoch 35/100\n",
      " 40/499 [=>............................] - ETA: 1:23 - loss: 4.2739 - accuracy: 0.1818"
     ]
    }
   ],
   "source": [
    "trainOrder = [\"PARSA\", \"JETT\", \"MILO\", \"MARS\", \"MILES\", \"GWYN\", \"ERIC\"]\n",
    "for n in trainOrder:\n",
    "    print(f\"Training {n} bot\")\n",
    "    trainModel(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
