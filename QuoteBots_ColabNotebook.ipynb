{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "QuoteBots-ColabNotebook.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oS6g5zmYDWHf",
        "colab_type": "text"
      },
      "source": [
        "**The bot versions of the Skype group**\n",
        "\n",
        "*Born from nothing by the genius, Eric, using only his notebooks and some of Google's GPUs, the bots have arrived. Each mirrors its owner to some infinitesimal degree*\n",
        "\n",
        "Special Awards:\n",
        "\n",
        "*    David-bot: The most coherent of the bunch\n",
        "\n",
        "*    Diego-bot: The most mopey\n",
        "\n",
        "*    Miles-bot & Gwyn-bot: Incoherent babbling about one another\n",
        "\n",
        "*    Milo-bot: Spent the most time in school, just like the real one\n",
        "\n",
        "*    Jett-bot: Talks most about videogames\n",
        "\n",
        "*    Parsa-bot: A massive D&D nerd\n",
        "\n",
        "*    Mars-bot: Offensive, bordering on actionable\n",
        "\n",
        "*    Eric-bot: So hard to replicate that the genius-bot actually killed all attempts at being trained\n",
        "\n",
        "*Disclaimer. Anything generated by the bots is solely on the AI and really only reflects on the friend whose data was used to train it, not on me*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WbpufpbVFcn_",
        "colab_type": "text"
      },
      "source": [
        "**IMPORTANT INSTRUCTIONS**\n",
        "\n",
        "Steps to run:\n",
        "\n",
        "1. Download and install git (google it)\n",
        "\n",
        "2. Open your terminal or command prompt and type \"git clone https://github.com/Leuyangh/quoteAIs.git\"\n",
        "\n",
        "3. Open the downloaded folder and find the \"FinalModels\" folder\n",
        "\n",
        "4. To the left of this notebook in Google colab, open up the file explorer and click \"Mount google drive\"\n",
        "\n",
        "5. Upload the FinalModels folder and make sure it is under \"My Drive\" and not a subfolder. Path should look like drive->My Drive->FinalModels\n",
        "\n",
        "6. Go to Runtime at the top here and click \"run all\"\n",
        "\n",
        "7. Find the bottom cell with `generateQuote(\"MILO\", 20, 10)` in it. That's the call that generates quotes. Either add new cells and run it with different names or just modify that call. Shift+Enter to run just that cell and save some time. \n",
        "\n",
        "8. Addendums \n",
        "\n",
        "> `generateQuote(\"NAME\" - name of bot to use, 10 - number here is how long the quote should be in words, 10 - number of quotes to generate)`\n",
        "\n",
        ">e.g. `generateQuote(\"MARS\", 50, 5)` will generate 5 quotes from Mars, each 50 words long\n",
        "\n",
        ">Alternatively, use `generateSentence(name)` to get a sentence from the Ngram model (statistical generation not a recurrent neural net) \n",
        ">>This only returns a string of the sentence generated, to print you need to use print(generateSentence(name))\n",
        "\n",
        "To update if I ever create a new model:\n",
        "\n",
        "1. Go into the directory made when you git cloned the repo\n",
        "\n",
        "2. run \"git pull\"\n",
        "\n",
        "3. Upload that new folder to drive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-iuzX9AIQ3e",
        "colab_type": "text"
      },
      "source": [
        "**Code, just ignore**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IKr7tjdY1Bha",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import string, os, random\n",
        "import warnings\n",
        "import h5py\n",
        "import datetime\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from numpy import loadtxt\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "from numpy.random import seed\n",
        "\n",
        "tensorflow.random.set_seed(int(datetime.datetime.utcnow().timestamp()))\n",
        "seed(int(datetime.datetime.utcnow().timestamp()))"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v2BPYIjF1uDh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loadTokenizer(name):\n",
        "    name = \"/content/drive/My Drive/FinalModels/\" + name + \"_tokenizer.pkl\"\n",
        "    tokenizer = pickle.load(open(name, 'rb'))\n",
        "    return tokenizer\n",
        "\n",
        "def loadFromFile(filename):\n",
        "    file = open(filename, 'r', encoding = \"utf-8\")\n",
        "    text = file.read()\n",
        "    file.close()\n",
        "    return text\n",
        "\n",
        "def load_quotegen(name):\n",
        "    return pickle.load(open(\"/content/drive/My Drive/FinalModels/\" + name + '.pkl', 'rb') )"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2n9WywN72lHk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generateQuote(name = 'PARSA', length = 30, count = 5):\n",
        "    in_filename = \"/content/drive/My Drive/FinalModels/\" + name + \"_sequences.txt\"\n",
        "    file = loadFromFile(in_filename)\n",
        "    lines = file.split('\\n')\n",
        "    tokenizer = loadTokenizer(name)\n",
        "    sequences = tokenizer.texts_to_sequences(lines)\n",
        "    sequences = np.array(sequences)\n",
        "    X, y = sequences[:,:-1], sequences[:,-1]\n",
        "    seq_length = X.shape[1]\n",
        "    model = load_model(\"/content/drive/My Drive/FinalModels/\" + name + '_model.h5')\n",
        "    for i in range(0, count):\n",
        "        seed_text = lines[np.random.randint(0,len(lines))]\n",
        "        result = list()\n",
        "        for _ in range(length):\n",
        "            # encode the text as integer\n",
        "            encoded = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "            # truncate sequences to a fixed length\n",
        "            encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
        "            # predict probabilities for each word\n",
        "            yhat = np.argmax(model.predict(encoded), axis = -1)\n",
        "            # map predicted word index to word\n",
        "            out_word = ''\n",
        "            for word, index in tokenizer.word_index.items():\n",
        "                if index == yhat:\n",
        "                    out_word = word\n",
        "                    break\n",
        "            # append to input\n",
        "            seed_text += ' ' + out_word\n",
        "            result.append(out_word)\n",
        "        final = ' '.join(result)\n",
        "        print(f'{name}: \"{final}\"')\n",
        "\n",
        "def generateQuoteSeeded(name = 'PARSA', input = \"\"):\n",
        "    tokenizer = loadTokenizer(name)\n",
        "    model = load_model(\"/content/drive/My Drive/FinalModels/\" + name + '_model.h5')\n",
        "    result = list()\n",
        "    lines = input.split()\n",
        "    for _ in range(len(input)):\n",
        "            # encode the text as integer\n",
        "            encoded = tokenizer.texts_to_sequences([lines])[0]\n",
        "            # truncate sequences to a fixed length\n",
        "            encoded = pad_sequences([encoded], maxlen=50, truncating='pre')\n",
        "            # predict probabilities for each word\n",
        "            yhat = np.argmax(model.predict(encoded), axis = -1)\n",
        "            # map predicted word index to word\n",
        "            out_word = ''\n",
        "            for word, index in tokenizer.word_index.items():\n",
        "                if index == yhat:\n",
        "                    out_word = word\n",
        "                    break\n",
        "            # append to input\n",
        "            lines += ' ' + out_word\n",
        "            result.append(out_word)\n",
        "    final = ' '.join(result)\n",
        "    print(f'{name}: \"{final}\"')\n",
        "\n",
        "def generate_sentence(name):\n",
        "    quote_gen = load_quotegen(name)\n",
        "    text = [None,None,None]\n",
        "    sentence_finished = False\n",
        "    while not sentence_finished:\n",
        "        r = random.random()\n",
        "        accumulator = .0\n",
        "\n",
        "        for word in quote_gen[tuple(text[-3:])].keys():\n",
        "            accumulator += quote_gen[tuple(text[-3:])][word]\n",
        "\n",
        "            if accumulator >= r:\n",
        "                text.append(word)\n",
        "                break\n",
        "\n",
        "        if text[-3:] == [None,None,None]:\n",
        "            sentence_finished = True\n",
        "\n",
        "    return ' '.join([t for t in text if t])"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxqRvlm5IVJm",
        "colab_type": "text"
      },
      "source": [
        "**Make calls here using instructions above**\n",
        "\n",
        "Shift+enter to run a cell. If any errors pop up make sure your drive is mounted and the folder is in there correctly. If any runtime errors occur try running all the previous cells with \"Runtime\"->run all or shift+enter in them one by one.\n",
        "\n",
        "Names should be capitalized"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EuE71wI7KaSa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "names = [\"DAVID\", \"DIEGO\", \"GWYN\", \"JETT\", \"MARS\", \"MILES\", \"MILO\", \"PARSA\"] #ERIC to come when I figure it out"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hrbJkrlJOLY-",
        "colab_type": "text"
      },
      "source": [
        "*Recommended that you set the second parameter to something long, like 20-50 at least and then pick out your favorite sentence from within it. Bot is likely to start generating based on text partway through a message so you want it long enough that the bot translates from the end of that message all the way through the end of the one after at least, likely will end with a few words into the next-next message or even next-next-next message*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YDYPnGIN1JDj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "fa50bedc-0902-46cc-e94f-421104579a1e"
      },
      "source": [
        "generateQuote(\"MILO\", 20, 10)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MILO: \"back in her reverted form i want a new champ and i think it was just a good change i\"\n",
            "MILO: \"who spoke little english told me that exact thing about me me ill change much like minutes ago we need\"\n",
            "MILO: \"a lunch with my phone on kro so i can make you to chillin on two seperate mustve no structure\"\n",
            "MILO: \"holy shit dude they dont get it at am good so i think sunday is poseidon kinda good skirmishers simulator\"\n",
            "MILO: \"its actually good just not wrong they dont understand right ik actually im saying yea i am sick xd and\"\n",
            "MILO: \"good yea yea fosho with one better team need to be deep idiot i think a chick gemis food and\"\n",
            "MILO: \"system flips a coin look at how this guy you favoring trumps inaction means he didnt get how it is\"\n",
            "MILO: \"a new league i could fit at but if its a good idea if you can do it poorly i\"\n",
            "MILO: \"in the pot not the end of the world you one what if there is that if it fails this\"\n",
            "MILO: \"something have you been at games of a young guy but its just a little worse its gonna be broken\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZhN4TI1lvI7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "c4a917c6-6529-4b1a-d2c3-73606616992d"
      },
      "source": [
        "for i in range(0, 10):\n",
        "  print(\"Parsa: \" + generate_sentence(\"PARSA\"))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Parsa: are you claiming that the lack of emotion\n",
            "Parsa: we have two people who belong to order\n",
            "Parsa: Bs it 's not racist\n",
            "Parsa: sitting on samecord btw\n",
            "Parsa: im back in santa monica\n",
            "Parsa: How about a Jedi trained by dumbledore to act as the starship enterprise â€™ s chief mechanic but ended up falling to the ruinous powers of chaos ?\n",
            "Parsa: ummm\n",
            "Parsa: I WANT ONE\n",
            "Parsa: is n't it just dark kaer morhen\n",
            "Parsa: oh hey\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ElKIbutU9ykd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "7562e30d-1c79-4bc4-ea64-c7badd0bbb21"
      },
      "source": [
        "for n in names:\n",
        "  generateQuote(n, 20, 1)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DAVID: \"nightmare on elm street issa nightmare on elm street a nightmare on elm street feel like jason friday hockey mask\"\n",
            "DIEGO: \"second mask edited previous message edge edgs i got this one too black is the new orange even more if\"\n",
            "GWYN: \"culture for a couple glass beads africa from define cultural appropriation in such a retarded manner taking elements of someones\"\n",
            "JETT: \"speaking of stupidity and food one of the likely reasons for their primitive brains is the fact that additionally to\"\n",
            "MARS: \"most wrong is not a good idea and you can basing it happen for a bunch of negusians or negusites\"\n",
            "MILES: \"see what youre typing michael so ill assume youre typing color because i cant see itmiles obrien today pm miles\"\n",
            "MILO: \"we love and it had to use it then no need to slouch on etiquette tho ah should be me\"\n",
            "PARSA: \"secure alliances to fight off a godlike entity in a neighboring island and an even more likely event is that\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_gS364TNxWZ",
        "colab_type": "text"
      },
      "source": [
        "This is just a thing I tried to do where you can feed a bot some sample thing and it'll convert to {person}'s version of it but I don't think its working right yet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_ULUg9RKrpY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "904bbec9-2f40-485e-88e2-2fc14b384aa1"
      },
      "source": [
        "generateQuoteSeeded(\"MILES\", \"hi hi hi\")"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MILES: \"do n naysh o m o t i\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wsdxJ9xTU4rY",
        "colab_type": "text"
      },
      "source": [
        "**Supplemental Info**\n",
        "\n",
        "GenerateQuote takes some time to start up but is quick to perform multiple predictions due to the overhead of loading in the files. Will try to fix that but it may be inevitable. To work around this, use the third parameter to generate a bunch of quotes at once\n",
        "\n",
        "The cell starting with \"for n in names\" is REALLY slow so don't hit run-all unless you want that to run. Just use shift-enter on the cell you do want"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tk4sG7ppVbbR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
