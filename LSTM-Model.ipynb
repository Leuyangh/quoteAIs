{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string, os \n",
    "import warnings\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import Sequential\n",
    "import tensorflow.keras.utils as ku \n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "from numpy.random import seed\n",
    "import tensorflow\n",
    "tensorflow.random.set_seed(4)\n",
    "seed(20)\n",
    "os.chdir(\"../quoteAIsData\")\n",
    "\n",
    "def writeToFile(filename, data):\n",
    "    file = open(filename, 'w', encoding = \"utf-8\")\n",
    "    file.write(data)\n",
    "    file.close()\n",
    "    \n",
    "def loadFromFile(filename):\n",
    "    file = open(filename, 'r', encoding = \"utf-8\")\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DAVID 108324\n",
      "DIEGO 106373\n",
      "ERIC 1249063\n",
      "GWYN 1312330\n",
      "JETT 181826\n",
      "MARS 695638\n",
      "MILES 791200\n",
      "MILO 886187\n",
      "PARSA 339260\n"
     ]
    }
   ],
   "source": [
    "quote_dict = pickle.load(open('quotes.pkl', 'rb'))\n",
    "corpus_dict = {}\n",
    "for k in quote_dict:\n",
    "    corpus = ' '.join(quote_dict[k])\n",
    "    corpus_dict[k] = corpus\n",
    "    print(k, len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DAVID\n",
      "Total Tokens: 19020\n",
      "Unique Tokens: 3960\n",
      "DIEGO\n",
      "Total Tokens: 19084\n",
      "Unique Tokens: 3077\n",
      "ERIC\n",
      "Total Tokens: 231264\n",
      "Unique Tokens: 15618\n",
      "GWYN\n",
      "Total Tokens: 149662\n",
      "Unique Tokens: 10075\n",
      "JETT\n",
      "Total Tokens: 33283\n",
      "Unique Tokens: 5291\n",
      "MARS\n",
      "Total Tokens: 128621\n",
      "Unique Tokens: 10492\n",
      "MILES\n",
      "Total Tokens: 121145\n",
      "Unique Tokens: 10124\n",
      "MILO\n",
      "Total Tokens: 163472\n",
      "Unique Tokens: 11676\n",
      "PARSA\n",
      "Total Tokens: 63888\n",
      "Unique Tokens: 7952\n",
      "DAVID - 18969 sequences\n",
      "DIEGO - 19033 sequences\n",
      "ERIC - 231213 sequences\n",
      "GWYN - 149611 sequences\n",
      "JETT - 33232 sequences\n",
      "MARS - 128570 sequences\n",
      "MILES - 121094 sequences\n",
      "MILO - 163421 sequences\n",
      "PARSA - 63837 sequences\n"
     ]
    }
   ],
   "source": [
    "token_dict = {}\n",
    "for key, corpus in corpus_dict.items():\n",
    "    # split into tokens by white space\n",
    "    tokens = corpus.split()\n",
    "    # remove punctuation from each token\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # make lower case\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    token_dict[key] = tokens\n",
    "    print(key)\n",
    "    print('Total Tokens: %d' % len(tokens))\n",
    "    print('Unique Tokens: %d' % len(set(tokens)))\n",
    "\n",
    "for key, tokens in token_dict.items(): \n",
    "    length = 51\n",
    "    sequences = list()\n",
    "    for i in range(length, len(tokens)):\n",
    "        seq = tokens[i-length:i]\n",
    "        line = ' '.join(seq)\n",
    "        sequences.append(line)\n",
    "    print(f\"{key} - {len(sequences)} sequences\")\n",
    "    writeToFile(key+\"_sequences.txt\", '\\n'.join(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel(name):\n",
    "    in_filename = name + \"_sequences.txt\"\n",
    "    file = loadFromFile(in_filename)\n",
    "    lines = file.split('\\n')\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    sequences = tokenizer.texts_to_sequences(lines)\n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "    print(f\"{name}: {vocab_size} vocabulary size\")\n",
    "    pickle.dump(tokenizer, open(name + '_tokenizer.pkl', 'wb'))\n",
    "    sequences = np.array(sequences)\n",
    "    X, y = sequences[:,:-1], sequences[:,-1]\n",
    "    y = np_utils.to_categorical(y, num_classes=vocab_size)\n",
    "    seq_length = X.shape[1]\n",
    "    # define model\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, 100, input_length=seq_length))\n",
    "    model.add(LSTM(200, return_sequences=True))\n",
    "    model.add(LSTM(200))\n",
    "    model.add(Dense(200, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(vocab_size, activation='softmax'))\n",
    "    print(model.summary())\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    # fit model\n",
    "    model.fit(X, y, batch_size=128, epochs=300)\n",
    "    model.save(name + '_char_word_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keepTraining(name, epochsCount):\n",
    "    in_filename = name + \"_sequences.txt\"\n",
    "    file = loadFromFile(in_filename)\n",
    "    lines = file.split('\\n')\n",
    "    modelname = name + \"_char_word_model.h5\"\n",
    "    model = load_model(modelname)\n",
    "    tokenizer = pickle.load(open(name + \"_tokenizer.pkl\", 'rb'))\n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "    sequences = tokenizer.texts_to_sequences(lines)\n",
    "    sequences = np.array(sequences)\n",
    "    X, y = sequences[:,:-1], sequences[:,-1]\n",
    "    y = np_utils.to_categorical(y, num_classes=vocab_size)\n",
    "    seq_length = X.shape[1]\n",
    "    print(model.summary())\n",
    "    model.fit(X, y, batch_size=128, epochs=epochsCount)\n",
    "    model.save(name + '_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parsa, mars, david, diego, Miles done\n",
    "trainOrder = [\"JETT\", \"MILO\", \"MILES\", \"GWYN\", \"ERIC\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
