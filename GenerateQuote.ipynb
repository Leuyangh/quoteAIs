{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string, os \n",
    "import warnings\n",
    "import h5py\n",
    "import datetime\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from numpy import loadtxt\n",
    "from tensorflow.keras.models import load_model\n",
    "#model = load_model('name.h5')\n",
    "\n",
    "from numpy.random import seed\n",
    "import tensorflow\n",
    "#tensorflow.random.set_seed(4)\n",
    "#seed(20) - original seeds\n",
    "tensorflow.random.set_seed(int(datetime.datetime.utcnow().timestamp()))\n",
    "seed(int(datetime.datetime.utcnow().timestamp()))\n",
    "os.chdir(\"../quoteAIsData\")\n",
    "names = [\"DAVID\", \"DIEGO\", \"ERIC\", \"GWYN\", \"JETT\", \"MARS\", \"MILES\", \"MILO\", \"PARSA\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadToken(name):\n",
    "    tokenizer = pickle.load(open(name + '_tokenizer.pkl', 'rb'))\n",
    "    return tokenizer\n",
    "\n",
    "def loadSequences(name):\n",
    "    file = open(name+'_sequences.txt', 'r', encoding = \"utf-8\")\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "def generateQuotes(name, wordCount, quoteCount):\n",
    "    tokenizer = loadToken(name)\n",
    "    lines = loadSequences(name)\n",
    "    lines = lines.split('\\n')\n",
    "    sequences = tokenizer.texts_to_sequences(lines)\n",
    "    sequences = np.array(sequences)\n",
    "    X, y = sequences[:,:-1], sequences[:,-1]\n",
    "    seq_length = X.shape[1]\n",
    "    model = load_model(name + '_char_word_model.h5')\n",
    "    for i in range(0, quoteCount):\n",
    "        seed_text = lines[np.random.randint(0,len(lines))]\n",
    "        result = list()\n",
    "        for _ in range(wordCount):\n",
    "            # encode the text as integer\n",
    "            encoded = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "            # truncate sequences to a fixed length\n",
    "            encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
    "            # predict probabilities for each word\n",
    "            yhat = model.predict_classes(encoded, verbose=0)\n",
    "            # map predicted word index to word\n",
    "            out_word = ''\n",
    "            for word, index in tokenizer.word_index.items():\n",
    "                if index == yhat:\n",
    "                    out_word = word\n",
    "                    break\n",
    "            # append to input\n",
    "            seed_text += ' ' + out_word\n",
    "            result.append(out_word)\n",
    "        final = ' '.join(result)\n",
    "        print(f'{name}: \"{final}\"')\n",
    "        filename = \"0_Results/\" + name + \".txt\"\n",
    "        file = open(filename, 'a', encoding = 'utf-8')\n",
    "        file.write(\"-\" + final + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DAVID: \"out it the fuck decide the song cuts ive the best captioned thot slayed it been looking for innumerable how i the t though i just know i didnt even know what happened i think it letter of the luger\"\n",
      "DAVID: \"social networking site for neighborhoods i was hoping the rumors on nextdoor werent true well i am more disturbing than a single ones well on no game totally when i havent still m r m s i cant post the\"\n",
      "DAVID: \"nothing i just like the friends s i cant trained in monkey again it not a meme i dont post assuming you cant finesse manage where i have to do it i dont know what it was i dont know\"\n",
      "DAVID: \"she was is fast rainbowsmile they know monkey a place on the song plus if i guess spoiled by the lack of ads i dont want to engage on the end thus rewatching it was more hahaa no salted is\"\n",
      "DAVID: \"no its recently how not a only i couldve felt i aint learned i dont eat ya god is true anything i dont know al players oddly thanks is then moral people a picture of to be better league yall\"\n",
      "DAVID: \"rate across the country the professor lost his tenure and was fired the next day he died of the gay plague aids and was tossed into the lake of fire for all eternity semper fi ps close the borders i\"\n",
      "DAVID: \"pepe weeb champs puke weeb champs puke never been more angry this w about k used m no video is two hours ive heard he bbut daehpuc apparently had a year for walking now i sit and beta are was\"\n",
      "DAVID: \"the family of called the pepperoni i show i gazed why he the top laner tears gave writing a case in america you know that it infrequently i cant find what the course is being happen im something league gottem\"\n",
      "DAVID: \"outer worlds shiet fuckin incels the real problem is this one plays me jour fields it is tripped of attack wot do yeah i dont know what soon of the things ive seen very bad hours in a time jk\"\n",
      "DAVID: \"did thats it like earthquake number is kevin lmao its not a actions would t c c b a wild she not say what no weeks manifesto or doesnt in reading the price i look at least hundreds of fellas\"\n"
     ]
    }
   ],
   "source": [
    "generateQuotes(\"DAVID\", 40, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DIEGO: \"machine broke eggcellent out for artifact of ax i knew it know that ok i can want to play out\"\n",
      "DIEGO: \"digbo knows but this play on his screams edited previous message i ハ ｖ i was awoken to run more\"\n",
      "DIEGO: \"legends im over i dont genuinely recommend that the trending part for the asian concise budget to manage in existence\"\n",
      "DIEGO: \"told me now i dont genuinely recommend it i am the lot or the dark icon for our placement guys\"\n",
      "DIEGO: \"im interested in participating for i think you can play on i didnt be to listening to the megalovania song\"\n",
      "DIEGO: \"of broccoli kicked in my door is a shotgun and redeem i drew temmie lilypichus discord champion i dont want\"\n",
      "DIEGO: \"ｏｒ ａｓｓａｕｌｔ ａｎｄ ｂａｔｔｅｒｙ ｌｉｋｅ ｙｏｕ ｍａｄｅ ｄｅｎｔｉｎｇ ｂｌｏｗｓ ｄｅｎｔｉｎｇ ｂｌｏｗｓ ｌｉｋｅ ｙｏｕ ａｓｓａｕｌｔｅｄ ｙｏｕｒ ｂｌｏｗｓ defender ｙｏｕ whats outdoor\"\n",
      "DIEGO: \"destructive me i dont think that janna youve can i blame me i can like the ending of wol im\"\n",
      "DIEGO: \"with the chain fap notlikethis i can want to get to drink before though legends preaching huang for the same\"\n",
      "DIEGO: \"for hrs to go get it going to show bobs and vagene edited previous message fire i l the number\"\n"
     ]
    }
   ],
   "source": [
    "generateQuotes(\"DIEGO\", 20, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
